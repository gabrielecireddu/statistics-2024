<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Shannon Entropy, Diversity Measures, and Primitive Roots</title>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <style>
        body {
            font-family: Arial, sans-serif;
            padding: 10px;
            margin: 0;
        }
        h1, h2 {
            font-size: 24px;
        }
        h3 {
            font-size: 20px;
        }
        p {
            font-size: 16px;
        }
        .math-container {
            max-width: 100%;
            overflow-x: auto;
        }
        /* Responsive resizing for formulas */
        @media (max-width: 1200px) {
            p {
                font-size: 12px;
            }
        }
        @media (max-width: 880px) {
            p {
                font-size: 10px;
            }
        }
        @media (max-width: 740px) {
            p {
                font-size: 8px;
            }
        }
        @media (max-width: 600px) {
            p {
                font-size: 6px;
            }
        }
        @media (max-width: 500px) {
            p {
                font-size: 5px;
            }
        }
        @media (max-width: 450px) {
            p {
                font-size: 4px;
            }
        }
        @media (max-width: 380px) {
            p {
                font-size: 3px;
            }
        }
        @media (max-width: 290px) {
            p {
                font-size: 2px;
            }
        }
        @media (max-width: 240px) {
            p {
                font-size: 1px;
            }
        }
    </style>
</head>
<body>
    <div class="math-container">
        <h1>Shannon Entropy, Diversity Measures, and Primitive Roots</h1>
        <p>
            Quantifying uncertainty and diversity within systems is a fundamental aspect of statistics and information theory. <strong>Shannon Entropy</strong> serves as a pivotal measure in this regard, offering insights into the unpredictability inherent in probability distributions. Concurrently, in the realm of number theory, the concept of <strong>primitive roots</strong> modulo prime numbers reveals essential properties about the multiplicative structures of integers under modular arithmetic.
        </p>
        <hr>
        <h2>Shannon Entropy and Diversity Measures</h2>
        <h3>1. Shannon Entropy</h3>
        <p><strong>Definition:</strong></p>
        <p>
            For a discrete random variable \( X \) with possible outcomes \( \{ x_1, x_2, \dots, x_n \} \) and corresponding probabilities \( P(x_i) \), the <strong>Shannon Entropy</strong> \( H(X) \) is defined as:
        </p>
        \[
        H(X) = -\sum_{i=1}^{n} P(x_i) \log_b P(x_i)
        \]
        <p>
            where \( b \) is the base of the logarithm, commonly 2, yielding entropy in bits.
        </p>
        <p><strong>Interpretation:</strong></p>
        <ul>
            <li><strong>Maximum Entropy:</strong> Occurs when all outcomes are equally likely, i.e., \( P(x_i) = \frac{1}{n} \) for all \( i \). In this case, \( H(X) = \log_b n \).</li>
            <li><strong>Minimum Entropy:</strong> Occurs when one outcome has probability 1, and all others have probability 0. Here, \( H(X) = 0 \), indicating complete certainty.</li>
        </ul>
        <p><strong>Properties:</strong></p>
        <ul>
            <li><strong>Non-negativity:</strong> \( H(X) \geq 0 \).</li>
            <li><strong>Additivity:</strong> For independent random variables \( X \) and \( Y \), \( H(X, Y) = H(X) + H(Y) \).</li>
            <li><strong>Concavity:</strong> Entropy is a concave function with respect to the probability distribution \( P(x_i) \).</li>
        </ul>
        <p><strong>Applications:</strong></p>
        <ul>
            <li><strong>Information Theory:</strong> Determines the theoretical limit for data compression (e.g., Huffman coding).</li>
            <li><strong>Cryptography:</strong> Assesses the unpredictability of cryptographic keys.</li>
            <li><strong>Statistical Mechanics:</strong> Relates to the disorder within physical systems.</li>
        </ul>
        <h3>2. Other Diversity Measures</h3>
        <h4>a. Rényi Entropy</h4>
        <p><strong>Definition:</strong></p>
        <p>
            For \( \alpha > 0 \) and \( \alpha \neq 1 \), the <strong>Rényi Entropy</strong> \( H_\alpha(X) \) is defined as:
        </p>
        \[
        H_\alpha(X) = \frac{1}{1 - \alpha} \log_b \left( \sum_{i=1}^{n} P(x_i)^\alpha \right)
        \]
        <p><strong>Interpretation:</strong></p>
        <ul>
            <li>Generalizes Shannon Entropy (recovered as \( \alpha \to 1 \)).</li>
            <li>Allows weighting of probabilities differently, emphasizing rare or common events based on \( \alpha \).</li>
        </ul>
        <h4>b. Simpson's Diversity Index</h4>
        <p><strong>Definition:</strong></p>
        <p>
            Used primarily in ecology, it is defined as:
        </p>
        \[
        D = 1 - \sum_{i=1}^{n} P(x_i)^2
        \]
        <p><strong>Interpretation:</strong></p>
        <ul>
            <li>Measures the probability that two individuals randomly selected from a sample belong to different categories.</li>
            <li>Ranges from 0 (no diversity) to a maximum value approaching 1 (infinite diversity).</li>
        </ul>
        <h4>c. Gini Coefficient</h4>
        <p><strong>Definition:</strong></p>
        <p>
            A measure of inequality among values of a frequency distribution, such as levels of income.
        </p>
        <p><strong>Interpretation:</strong></p>
        <ul>
            <li>Ranges from 0 (perfect equality) to 1 (perfect inequality).</li>
            <li>Calculated based on the differences between every possible pair of values.</li>
        </ul>
        <hr>
        <h2>Primitive Roots Modulo Prime Numbers</h2>
        <h3>1. Fundamental Concepts</h3>
        <p><strong>Definition:</strong></p>
        <p>
            Given a prime number \( p \), a number \( g \) is called a <strong>primitive root modulo \( p \)</strong> if:
        </p>
        <ul>
            <li>\( g \) and \( p \) are coprime (\( \gcd(g, p) = 1 \)).</li>
            <li>For every integer \( a \) such that \( 1 \leq a < p \) and \( \gcd(a, p) = 1 \), there exists an integer \( k \) satisfying:
            \[
            g^k \equiv a \mod p
            \]
            </li>
        </ul>
        <p><strong>Properties:</strong></p>
        <ul>
            <li>The set \( \{1, 2, \dots, p - 1\} \) forms a multiplicative group under modulo \( p \).</li>
            <li>This group is cyclic, and the primitive root \( g \) serves as a generator.</li>
            <li>The number of primitive roots modulo \( p \) is given by Euler's totient function \( \phi(\phi(p)) = \phi(p - 1) \).</li>
        </ul>
        <h3>2. Examples</h3>
        <p><strong>Example 1: \( p = 7 \)</strong></p>
        <p>Possible residues: \( \{1, 2, 3, 4, 5, 6\} \).</p>
        <p>Testing \( g = 3 \):</p>
        \[
        \begin{align*}
        3^1 &\equiv 3 \mod 7 \\
        3^2 &\equiv 2 \mod 7 \\
        3^3 &\equiv 6 \mod 7 \\
        3^4 &\equiv 4 \mod 7 \\
        3^5 &\equiv 5 \mod 7 \\
        3^6 &\equiv 1 \mod 7 \\
        \end{align*}
        \]
        <p>
            Powers of 3 generate all residues modulo 7, so 3 is a primitive root modulo 7.
        </p>
        <p><strong>Example 2: \( p = 11 \)</strong></p>
        <p>Testing \( g = 2 \):</p>
        <p>
            Powers of 2 modulo 11 generate the sequence \( \{2, 4, 8, 5, 10, 9, 7, 3, 6, 1\} \).
        </p>
        <p>
            2 is a primitive root modulo 11.
        </p>
        <h3>3. Significance in Number Theory</h3>
        <ul>
            <li><strong>Cyclic Groups:</strong> The multiplicative group of integers modulo \( p \) is cyclic if \( p \) is prime.</li>
            <li><strong>Discrete Logarithms:</strong> Primitive roots are essential in defining discrete logarithms, which have applications in cryptography.</li>
            <li><strong>Primitive Element Theorem:</strong> Ensures the existence of primitive roots for prime moduli.</li>
        </ul>
        <hr>
        <h2>Applications and Connections</h2>
        <h3>1. Cryptography</h3>
        <ul>
            <li><strong>Diffie-Hellman Key Exchange:</strong> Relies on the difficulty of computing discrete logarithms in cyclic groups generated by primitive roots.</li>
            <li><strong>RSA Algorithm:</strong> Utilizes properties of modular arithmetic and the difficulty of factorization.</li>
        </ul>
        <h3>2. Information Theory and Number Theory Intersection</h3>
        <p>
            While Shannon Entropy and primitive roots originate from different mathematical areas, both deal with underlying structures that govern complexity and predictability.
        </p>
        <ul>
            <li><strong>Entropy in Cryptography:</strong> High entropy is crucial for secure key generation, which often employs number-theoretic functions involving primitive roots.</li>
        </ul>
        <h3>3. Statistical Mechanics and Randomness</h3>
        <ul>
            <li><strong>Entropy Measures:</strong> Provide insights into the randomness and disorder within systems.</li>
            <li><strong>Primitive Roots and Random Sequences:</strong> Used in pseudorandom number generation algorithms, contributing to statistical sampling methods.</li>
        </ul>
        <hr>
    </div>
</body>
</html>
